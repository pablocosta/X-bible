{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "#cerrega todo os arquivos\n",
    "dfsNew = []\n",
    "dfsOld = []\n",
    "\n",
    "for arq in os.listdir(\"./data/ENG/\"):\n",
    "    if arq.find(\"New\") > 0:\n",
    "        dfsNew.append(pd.read_csv(\"./data/ENG/\"+arq, sep=\"\\t\"))\n",
    "    elif arq.find(\"Old\") > 0:\n",
    "        dfsOld.append(pd.read_csv(\"./data/ENG/\"+arq, sep=\"\\t\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "from sacremoses import MosesTokenizer\n",
    "\n",
    "def tokenize(text, lang, nGram=3):\n",
    "    entok = MosesTokenizer(lang=lang)\n",
    "    text = entok.tokenize(text, escape=False)\n",
    "    grams = []\n",
    "    for i in range(1, nGram):\n",
    "        i_grams = [\n",
    "            \" \".join(gram)\n",
    "            for gram in ngrams(text, i)\n",
    "        ]\n",
    "        grams.extend(i_grams)\n",
    "        \n",
    "    return grams\n",
    "\n",
    "def getNgramOverlap(hypothesys, references, nGram, lang):\n",
    "\n",
    "  overlaps = []\n",
    "  for h, r in zip([hypothesys], [references]):\n",
    "    if (h == \"\") or (r == \"\"):\n",
    "      overlaps.append(1.0)\n",
    "      continue\n",
    "    a = tokenize(h, lang, nGram)\n",
    "    b = tokenize(r, lang, nGram)\n",
    "\n",
    "    if len(a) >= len(b):\n",
    "      overlaps.append(len(set(a) & set(b))/len(a))\n",
    "    elif len(b) >= len(a):\n",
    "      overlaps.append(len(set(a) & set(b))/len(b))\n",
    "\n",
    "  return overlaps[0]\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def getStats(dfA, dfB, lang):\n",
    "    df = dfA.merge(dfB, on=[\"livro\", \"capitulo\", \"versiculo\"])\n",
    "\n",
    "    df[\"sourceLen\"] = df[\"texto_x\"].apply(lambda x: getSizeSentece(x))\n",
    "    df[\"targetLen\"] = df[\"texto_y\"].apply(lambda x: getSizeSentece(x))\n",
    "    \n",
    "    df[\"overlap\"] = df.apply(lambda x: getNgramOverlap(x[\"texto_x\"], x[\"texto_y\"], 3, lang), axis=1)\n",
    "\n",
    "    return df\n",
    "#Matriz do novo testamento\n",
    "\n",
    "def getSizeSentece(text):\n",
    "    try:\n",
    "        return len(text.split(\" \"))\n",
    "    except:\n",
    "        return 0\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Novo testamento: \n"
     ]
    }
   ],
   "source": [
    "print(\"Novo testamento: \")\n",
    "newConcat = []\n",
    "for dfA in dfsNew:\n",
    "    dfsNew = dfsNew[1:]\n",
    "    for dfB in dfsNew:\n",
    "        newConcat.append(getStats(dfA, dfB, \"en\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tamanho sem cortes:  (3127932, 10)\n",
      "tamanho com cortes:  (3040910, 10)\n"
     ]
    }
   ],
   "source": [
    "dfConcat = pd.concat(newConcat, ignore_index=False)\n",
    "print(\"tamanho sem cortes: \", dfConcat.shape)\n",
    "#remove versiculos deseconhecidos\n",
    "dfConcat = dfConcat[dfConcat[\"versiculo\"] <= 180]\n",
    "#remove textos muito difenretes\n",
    "dfConcat = dfConcat[dfConcat[\"overlap\"] > 0.02]\n",
    "#filtra sentencas curtas\n",
    "dfConcat = dfConcat[dfConcat[\"sourceLen\"] > 5]\n",
    "dfConcat = dfConcat[dfConcat[\"targetLen\"] > 5]\n",
    "\n",
    "print(\"tamanho com cortes: \", dfConcat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfConcat.to_csv(\"./data/ENG/NovoTestamentoCompleto-ENG.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Velho testamento: \n"
     ]
    }
   ],
   "source": [
    "print(\"Velho testamento: \")\n",
    "oldConcat = []\n",
    "for dfA in dfsOld:\n",
    "    dfsOld = dfsOld[1:]\n",
    "    for dfB in dfsOld:\n",
    "        oldConcat.append(getStats(dfA, dfB, \"en\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tamanho sem cortes:  (10246620, 10)\n",
      "tamanho com cortes:  (8841448, 10)\n"
     ]
    }
   ],
   "source": [
    "dfConcat = pd.concat(oldConcat, ignore_index=False)\n",
    "print(\"tamanho sem cortes: \", dfConcat.shape)\n",
    "#remove versiculos deseconhecidos\n",
    "dfConcat = dfConcat[dfConcat[\"versiculo\"] <= 180]\n",
    "#remove textos muito difenretes\n",
    "dfConcat = dfConcat[dfConcat[\"overlap\"] > 0.02]\n",
    "#filtra sentencas curtas\n",
    "dfConcat = dfConcat[dfConcat[\"sourceLen\"] > 5]\n",
    "dfConcat = dfConcat[dfConcat[\"targetLen\"] > 5]\n",
    "\n",
    "print(\"tamanho com cortes: \", dfConcat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfConcat.to_csv(\"./data/ENG/VelhoTestamentoCompleto-ENG.tsv\", sep=\"\\t\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
